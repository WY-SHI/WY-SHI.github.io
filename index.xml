<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NeuroTech</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>NeuroTech</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â©2023 [WY-SHI](https://github.com/WY-SHI) All Rights Reserved.</copyright><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/logo_huda05752988ecfa1b9f15fa4d8dec8895_80551_300x300_fit_lanczos_2.png</url>
      <title>NeuroTech</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Example Page 1</title>
      <link>/courses/example/example1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/example/example1/</guid>
      <description>&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Page 2</title>
      <link>/courses/example/example2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/example/example2/</guid>
      <description>&lt;p&gt;Here are some more tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-3&#34;&gt;Tip 3&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-4&#34;&gt;Tip 4&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Brainnetome Atlas of Preadolescent Children based on Anatomical Connectivity Profiles</title>
      <link>/publication/cc2022_wl/</link>
      <pubDate>Tue, 18 Oct 2022 00:00:00 +0000</pubDate>
      <guid>/publication/cc2022_wl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Two Subtypes of Schizophrenia Identified by an Individual-level Atypical Pattern of Tensor-based Morphometric Measurement</title>
      <link>/publication/cc2022_wyshi/</link>
      <pubDate>Thu, 25 Aug 2022 00:00:00 +0000</pubDate>
      <guid>/publication/cc2022_wyshi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AnteriorâPosterior Hippocampal Dynamics Support Working Memory Processing</title>
      <link>/publication/jon2022_jl/</link>
      <pubDate>Wed, 19 Jan 2022 00:00:00 +0000</pubDate>
      <guid>/publication/jon2022_jl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Developing Neuroimaging Biomarker for Brain Diseases with a Machine Learning Framework and the Brainnetome Atlas</title>
      <link>/publication/neuroscibull2021_wyshi/</link>
      <pubDate>Mon, 14 Jun 2021 00:00:00 +0000</pubDate>
      <guid>/publication/neuroscibull2021_wyshi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reading and Tool List</title>
      <link>/post/cpti/</link>
      <pubDate>Sat, 23 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/post/cpti/</guid>
      <description>&lt;p&gt;Collection of Papers, Tools and Ideas focusing on neuroscience and machine learning which are worth spreading.&lt;/p&gt;
&lt;h3 id=&#34;1-papers&#34;&gt;1. Papers&lt;/h3&gt;
&lt;h4 id=&#34;11-mental-disorders&#34;&gt;1.1. Mental Disorders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;[AD] Qiu S, Joshi P S, Miller M I, et al. Development and validation of an interpretable deep learning framework for Alzheimerâs disease classification[J]. Brain, 2020.&lt;/li&gt;
&lt;li&gt;[AD] Varol E, Sotiras A, Davatzikos C, et al. HYDRA: Revealing heterogeneity of imaging and genetic patterns through a multiple max-margin discriminative analysis framework[J]. Neuroimage, 2017, 145: 346-364.&lt;/li&gt;
&lt;li&gt;[Survey] Zhang L, Wang M, Liu M, et al. A Survey on Deep Learning for Neuroimaging-based Brain Disorder Analysis[J]. arXiv preprint arXiv:2005.04573, 2020.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;12-neuroscience&#34;&gt;1.2. Neuroscience&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Yang G R, Joglekar M R, Song H F, et al. Task representations in neural networks trained to perform many cognitive tasks[J]. Nature neuroscience, 2019, 22(2): 297-306.&lt;/li&gt;
&lt;li&gt;Finn E S, Shen X, Scheinost D, et al. Functional connectome fingerprinting: identifying individuals using patterns of brain connectivity[J]. Nature neuroscience, 2015, 18(11): 1664-1671.&lt;/li&gt;
&lt;li&gt;Cai B, Zhang G, Zhang A, et al. Functional connectome fingerprinting: Identifying individuals and predicting cognitive functions via autoencoder[J]. Human Brain Mapping, 2021, 42(9): 2691-2705.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;13-graph-representation&#34;&gt;1.3. Graph Representation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Sankar A , Wu Y , Gou L , et al. Dynamic Graph Representation Learning via Self-Attention Networks[J]. 2018.&lt;/li&gt;
&lt;li&gt;Rossi E, Chamberlain B, Frasca F, et al. Temporal Graph Networks for Deep Learning on Dynamic Graphs[J]. arXiv preprint arXiv:2006.10637, 2020.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;14-interpretable-machine-learning&#34;&gt;1.4. Interpretable Machine Learning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Wan A, Dunlap L, Ho D, et al. NBDT: Neural-Backed Decision Trees[J]. arXiv preprint arXiv:2004.00221, 2020.&lt;/li&gt;
&lt;li&gt;Lundberg S M, Nair B, Vavilala M S, et al. Explainable machine-learning predictions for the prevention of hypoxaemia during surgery[J]. Nature Biomedical Engineering, 2018, 2(10): 749.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;15-unsupervised-learning&#34;&gt;1.5. Unsupervised Learning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Dong J, Cong Y, Sun G, et al. What can be transferred: Unsupervised domain adaptation for endoscopic lesions segmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 4023-4032.&lt;/li&gt;
&lt;li&gt;He K, Fan H, Wu Y, et al. Momentum contrast for unsupervised visual representation learning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 9729-9738.&lt;/li&gt;
&lt;li&gt;Hu H, Xie L, Hong R, et al. Creating Something from Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 3123-3132.&lt;/li&gt;
&lt;li&gt;Lin Y, Xie L, Wu Y, et al. Unsupervised person re-identification via softened similarity learning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 3390-3399.&lt;/li&gt;
&lt;li&gt;Tang H, Chen K, Jia K. Unsupervised Domain Adaptation via Structurally Regularized Deep Clustering[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 8725-8735.&lt;/li&gt;
&lt;li&gt;Wang D, Zhang S. Unsupervised Person Re-identification via Multi-label Classification[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 10981-10990.&lt;/li&gt;
&lt;li&gt;Ye M, Shen J. Probabilistic structural latent representation for unsupervised embedding[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 5457-5466.&lt;/li&gt;
&lt;li&gt;Yu Y, Odobez J M. Unsupervised Representation Learning for Gaze Estimation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 7314-7324.&lt;/li&gt;
&lt;li&gt;Zhan X, Xie J, Liu Z, et al. Online Deep Clustering for Unsupervised Representation Learning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 6688-6697.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2-tools&#34;&gt;2. Tools&lt;/h3&gt;
&lt;h4 id=&#34;21-freesurfer&#34;&gt;2.1. Freesurfer&lt;/h4&gt;
&lt;p&gt;FreeSurfer is a software package for the analysis and visualization of structural and functional neuroimaging data from cross-sectional or longitudinal studies. [
&lt;a href=&#34;http://surfer.nmr.mgh.harvard.edu/fswiki/FreeSurferWiki/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WiKi&lt;/a&gt;]&lt;/p&gt;
&lt;h4 id=&#34;22-fsl&#34;&gt;2.2. FSL&lt;/h4&gt;
&lt;p&gt;FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data. [
&lt;a href=&#34;https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WiKi&lt;/a&gt;]&lt;/p&gt;
&lt;h4 id=&#34;23-brant&#34;&gt;2.3. BRANT&lt;/h4&gt;
&lt;p&gt;BRAinNetome fMRI Toolkit &amp;ndash; MATLAB scripts/GUIs for fMRI data pre-/post-processes [
&lt;a href=&#34;https://github.com/kbxu/brant/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;] [
&lt;a href=&#34;http://brant.brainnetome.org/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Document&lt;/a&gt;]&lt;/p&gt;
&lt;h4 id=&#34;24-clinica&#34;&gt;2.4. Clinica&lt;/h4&gt;
&lt;p&gt;Software platform for clinical neuroimaging studies [
&lt;a href=&#34;https://github.com/aramis-lab/clinica&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt;
&lt;h4 id=&#34;25-nilearn&#34;&gt;2.5 Nilearn&lt;/h4&gt;
&lt;p&gt;Machine learning for Neuro-Imaging in Python [
&lt;a href=&#34;http://nilearn.github.io/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Document&lt;/a&gt;]&lt;/p&gt;
&lt;h4 id=&#34;26-dipy&#34;&gt;2.6 Dipy&lt;/h4&gt;
&lt;p&gt;DIPY is the paragon 3D/4D+ imaging library in Python. Contains generic methods for spatial normalization, signal processing, machine learning, statistical analysis and visualization of medical images. Additionally, it contains specialized methods for computational anatomy including diffusion, perfusion and structural imaging. [
&lt;a href=&#34;https://dipy.org/tutorials/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorials&lt;/a&gt;]&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>åçç£åå²</title>
      <link>/post/semi-supervised-segmentation/</link>
      <pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/semi-supervised-segmentation/</guid>
      <description>&lt;p&gt;å½åéå¯¹åçç£è¯­ä¹åå²é¢åï¼å¨ä¸åæ°æ®éä¸ï¼SOTAçæ¹æ³ä¸»è¦åå«CutMix, ClassMixä»¥ås4GAN_MLMT(æ°æ®æ¥æº
&lt;a href=&#34;https://paperswithcode.com/task/semi-supervised-semantic-segmentation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paperWithCode&lt;/a&gt;)ï¼backboneä¸»è¦éç¨DeepLabç³»å(v3+æv2).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Consistency Regularization:&lt;/strong&gt; predictions for unlabelled data should be invariant to perturbations.&lt;/p&gt;
&lt;p&gt;æå¤§åç±»åembeddingçç¸ä¼¼åº¦(multi domain).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;åå²backbone:&lt;/p&gt;
&lt;p&gt;â    a. DeepLabç³»å - V3: atrous convolutions + spatial pyramid pooling&lt;/p&gt;
&lt;p&gt;â    b. Encoder-decoder networks ç³»å - skip connections&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1-cutmix-githubhttpsgithubcombritefurycutmix-semisup-seg&#34;&gt;1 CutMix (
&lt;a href=&#34;https://github.com/Britefury/cutmix-semisup-seg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;French G , Laine S , Aila T , et al. Semi-supervised semantic segmentation needs strong, varied perturbations[J]. 2019.&lt;/p&gt;
&lt;p&gt;#Network Regularization, #Augmentation Strategies, #Mean Teacher Framework&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;11-åè¨æå½&#34;&gt;1.1 åè¨æå½&lt;/h3&gt;
&lt;p&gt;ç½ç»æ­£åå(Network Regularization)çææ³å¨äºè¦æ±ç½ç»å¯¹ä¸åå½¢å¼æ°å¨åå¾ç(æªæ æ³¨)è¾åºä¸è´æ§çé¢æµç»æ(æ´åæ¯ä¸ç§æ°æ®å¢å¹¿ææ¯).&lt;/p&gt;
&lt;p&gt;å¹³æ»æ§åè®¾:ç¸è¿çå¾çåºè¯¥å·å¤ç¸åçæ æ³¨ï¼èç±»åè®¾: å³ç­é¢åºè¯¥ä½äºæ°æ®åå¸å¯åº¦ç¸å¯¹è¾ä½çåºåï¼&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;MixUp, Cutoutä»¥åCutMix&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;MixUp: The inputs and target labels of two randomly chosen examples are blended using a randomly chosen
factor. å°ä¸¤å¼ å¾çè¿è¡ç»åï¼ä¾å¦$C=\alpha{A}+(1-\alpha){B}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cutout: Augment an image by masking a rectangular region to zero. å°å¾ççé¨ååºåèµå¼ä¸º0.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CutMix: combines aspects of MixUp and CutOut, cutting a rectangular region from image B and pasting it over image A.å°å¾ç$B$çé¨ååºåç²è´´è³å¾ç$A$ï¼å½¢ææ°çå¾çã&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Semi-supervised classification(ä¸»è¦è®¨è®ºåºäºä¸è´æ§æ­£ååçåçç£åç±»æ¹æ³)&lt;/p&gt;
&lt;p&gt;ç»åçç£æå¤±(ä¾å¦äº¤åçµ)åæ çç£ä¸è´æ§æå¤±&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ä¸åæ°å¨åé¢æµç»æçä¸è´æ§ï¼å½åé¢æµä¸åå²é¢æµçä¸è´æ§&lt;/li&gt;
&lt;li&gt;Mean teacher model éè¿çº¦æå­¦çç½ç»åæå¸ç½ç»é¢æµä¹é´çä¸è´æ§&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GAN-based adversarial learning: æå°åçå®æ ç­¾ä¸é¢æµæ ç­¾åå¸çå·®å¼&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;è¯­ä¹åå²ä¸­çä¸è´æ§æ­£åå/çº¦æ(Consistency regularization):&lt;/p&gt;
&lt;p&gt;åç±»ä¸­çä¸è´æ§çº¦æï¼$L_{cons}=d(f_{\theta}(x),f_{\theta}(\hat{x}))$ï¼å¶ä¸­$\hat{x}$ä¸ºå¯¹$x$æ½å æ°å¨åçå¾åï¼$d(Â·)$ä¸ºè·ç¦»åº¦éï¼&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;12-ä¸»è¦æ¹æ³&#34;&gt;1.2 ä¸»è¦æ¹æ³&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æ¨¡åæ¡æ¶&lt;/p&gt;
&lt;p&gt;ç»å¸çæ°å¨æ¹å¼ï¼å¦è£åªãç¼©æ¾ãæè½¬åé¢è²ååç­ï¼å¯¹è¾åºç±»é ææ··æ·çå çå¾ä½ï¼ä¹è¢«è¯æå¯¹æé«èªç¶å¾åçåç±»åç¡®çååææï¼åæ¶è¯¥æ¹æ³å¨ä¸äºå»å­¦å¾ååå²é®é¢ä¸ä¹ææ­£é¢çææï¼ä½æ¯å®å¯¹èªç¶å¾åçåå²ä»»å¡å´æ æã&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Cutoutæå©äºæåç½ç»å¯¹åç§ç¹å¾çææå©ç¨è½åï¼ä»èåæå¾åä¸åè¯­ä¹æåçå¤æ ·æ§ç»åã&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;çæäºå¼åmask Mï¼éæºéæ©ä¸ä¸ªç©å½¢èå´èµå¼ä¸º0 (éæºéæ©ç©å½¢çå¤§å°åé¿å®½æ¯)&lt;/li&gt;
&lt;li&gt;å°åå§å¾å$A$è¾å¥æå¸ç½ç»$g_{\phi}$æ¥äº§çä¼ªæ ç­¾(pseudo-targets)ï¼&lt;/li&gt;
&lt;li&gt;å©ç¨ä¼ªæ ç­¾åæ°å¨åçå¾å$\hat{x}$æ¥è®­ç»å­¦çç½ç»$f_{\theta}$&lt;/li&gt;
&lt;li&gt;è®¡ç®ä¸è´æ§æå¤±$L_{cons}=||M{\odot}(f_{\theta}(M{\odot}x)-g_{\phi}(x))||^2$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;Cutout.png&#34; alt=&#34;image-20201029160709707&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CutMix&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;åå§å¾å$x_a$å$x_b$,Mask $M$ (é¢ç§¯ä¸ºå¾åçä¸åï¼éæºé¿å®½æ¯åä½ç½®)&lt;/li&gt;
&lt;li&gt;å°åå§å¾åå±äºæå¸ç½ç»äº§çä¼ªæ ç­¾$g_{\phi}(x_a)$,$g_{\phi}(x_b)$&lt;/li&gt;
&lt;li&gt;å®ä¹$mix(Â·)$å½æ°ï¼$mix(a,b,M)=(1-M){\odot}a+M{\odot}b$&lt;/li&gt;
&lt;li&gt;è®¡ç®ä¸è´æ§æå¤±$L_{cons}=||mix(g_{\phi}(x_a),g_{\phi}(x_b),M)-f_{\theta}(mix(x_a, x_b,M))||^2$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;CutMix.png&#34; alt=&#34;image-20201029160757506&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;13-training-setup&#34;&gt;1.3 Training Setup&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;segmentation networks:
&lt;ol&gt;
&lt;li&gt;DeepLab v2 network based on ImageNet pre-trained ResNet-101;&lt;/li&gt;
&lt;li&gt;Dense U-net based on DensetNet-161;&lt;/li&gt;
&lt;li&gt;DeepLab v3+;&lt;/li&gt;
&lt;li&gt;PSPNet&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Loss function:
&lt;ul&gt;
&lt;li&gt;ç»åçç£æå¤±åä¸è´æ§æå¤±ï¼$L_{sup}+L_{cons}$&lt;/li&gt;
&lt;li&gt;ä¸ºä¿è¯ä¸¤é¡¹æå¤±keep balance,$L_{cons}$å¨ç±»å«ç»´åº¦æ±åï¼å¨ç©ºé´ç»´åº¦æ±å¹³å&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-classmix-githubhttpsgithubcomwilhelmtclassmix&#34;&gt;2 ClassMix (
&lt;a href=&#34;https://github.com/WilhelmT/ClassMix&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Olsson, Viktor, et al. &amp;ldquo;ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning.&amp;rdquo; &lt;em&gt;arXiv preprint arXiv:2007.07936&lt;/em&gt; (2020).&lt;/p&gt;
&lt;p&gt;#Network Regularization, #Augmentation Strategies, #Pseudo-labelling, #Mean Teacher Framework&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;ä¸ªäººæè§æ¹æ³ä¾§éäºå®ä¾åå²&lt;/em&gt;ï¼æåºäºä¸ç§æ°çæ°æ®å¢å¹¿ææ¯ClassMix&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ClassMix.png&#34; alt=&#34;image-20201029162223879&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;21-å¯¹æ¯cutmixä¸classmix&#34;&gt;2.1 å¯¹æ¯CutMixä¸ClassMix&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;CutMix: randomized rectangular regions are cut out from one image and pasted onto another. (mask-based mixing)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ClassMix (a generalization of CutMix): makes use of segmentations to generate the binary masks, instead of rectangles. (segmentation-based augmentation strategies)&lt;/p&gt;
&lt;p&gt;å¨çæmaskæ¶åºååæ¯åèæ¯&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;åºäºDeepLab-v2çClassMixä¼äºCutMix (ä½é¢è®­ç»æ°æ®ä¸å)ï¼æè¡æ¦ä¸æªæä¾ClassMixå©ç¨V3+çç»æ&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;compare.png&#34; alt=&#34;image-20201030092903981&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;22-æ¹æ³&#34;&gt;2.2 æ¹æ³&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ClassMix: å¦ä¸å¾æç¤ºï¼è¾å¥ä¸¤å¼ æªæ æ³¨çå¾ç$A$å$B$ï¼è¾åºä¸å¼ åæå¾ç$X_A$ä»¥åå¶ä¼ªæ ç­¾$Y_A$ï¼å°ä¸å¼ å¾ççåæ¯é¨å(éæºéæ©åå²åçä¸åæ°éçç±»å«)ç²è´´å°å¦ä¸å¼ å¾çä¸ï¼ä¼ªä»£ç å¦ä¸å¾æç¤ºï¼&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ClassMix_Alg.png&#34; alt=&#34;image-20201029184107086&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mean-Teacher Framework(a trend in state-of-the-art semi-supervised learning)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æå¤±å½æ°ï¼
$$
L(\theta)=E[l(f_{\theta}(X_L),Y_L)+{\lambda}l(f_{\theta}(X_A),Y_A)]
$$
å¶ä¸­$X_L$ä¸ºæ°æ®éä¸­å·²æ æ³¨çæ°æ®ï¼$X_A$ä¸ºå©ç¨æªæ æ³¨æ°æ®æ ¹æ®ClassMixåæçæ°æ®ï¼$l(Â·)$ä¸ºäº¤åçµæå¤±&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;3-s4gan_mlmt-githubhttpsgithubcomsud0301semisup-semseg&#34;&gt;3 S4GAN_MLMT (
&lt;a href=&#34;https://github.com/sud0301/semisup-semseg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Mittal S , Tatarchenko M , Brox T . Semi-Supervised Semantic Segmentation with High- and Low-level Consistency[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019, PP(99):1-1.&lt;/p&gt;
&lt;p&gt;#dual-branch method, #Mean Teacher Framework, #Network Fusion&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;errorSeg.png&#34; alt=&#34;image-20201029215233752&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;31-æ´ä½æ¡æ¶&#34;&gt;3.1 æ´ä½æ¡æ¶&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Dual-branch&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GAN-based branch (Semi-Supervised Semantic Segmentation GAN, s4GAN): è§£å³åå²é®é¢ä¸­çä½çº§éè¯¯(éè¯¯çå½¢ç¶ãä¸åç¡®çè¾¹çåä¸è¿è´¯çåå²ï¼å¦ä¸å¾c)ï¼å©ç¨å¤å«å¨å¤å«çå®çæ ç­¾ä»¥åç½ç»é¢æµçæ ç­¾(feature matching loss)ï¼æ§è¡åå²ä»»å¡(åç´ çº§åç±»)&lt;/li&gt;
&lt;li&gt;Semi-supervised multi-label classification branch (Multi-Label Mean Teacher, MLMT): éè¿å¤æ­å¾åä¸­åºç°çææç±»å«æ¥è§£å³åå²ä¸­å¯è½åºç°çæ ç­¾ç±»å«éè¯¯(å¦ä¸å¾d)ï¼æ§è¡å¾ççº§åç±»ã&lt;/li&gt;
&lt;li&gt;Network Fusionï¼èåç©ºé´ä¿¡æ¯åç±»ä¿¡æ¯(ç±»ä¼¼äºééæ³¨æåæºå¶)ï¼MLMTåæ¯äº§ççå¾åçº§ç±»å«æ ç­¾ç¨äºfilter s4GANçè¾åºã&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;s4GAN_MLMT.png&#34; alt=&#34;image-20201029221455524&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GANä¸­çdiscriminatorè¾åºå¯ä»¥è¢«å½ä½ä¸ä¸ªè´¨éè¯ä¼°æ¨¡åï¼ç¨äºéæ©æå¥½çé¢æµç»æç¨äºåç»­çself-trainingï¼&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;32-æ¹æ³&#34;&gt;3.2 æ¹æ³&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;s4GAN for Semantic Segmentation&lt;/p&gt;
&lt;p&gt;åå²ç½ç»$S$äº§çåå²ç»æï¼concatenateåå§å¾åä¸åå²ç»æéå¥å¤å«å¨æ¥å¯¹é½çå®æ æ³¨ä¸çæçåå²å¾çåå¸ã&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;è®­ç»åå²ç½ç»$S$: æå¤±å½æ°åæ¬ä¸é¨å(äº¤åçµæå¤±ï¼feature matchingæå¤±ï¼self-trainingæå¤±)ï¼&lt;/p&gt;
&lt;p&gt;a. äº¤åçµæå¤±ç¨äºåç´ çº§å«åç±»(åå²)&lt;/p&gt;
&lt;p&gt;b. feature matchingæå¤±$L_{fm}$ç¨äºæå°åçå®æ æ³¨ä¸é¢æµåå²å¾çmean discrepancy, å¶ä¸­$D_k(Â·)$ä»£è¡¨å¤å«å¨ç¬¬$k$å±çè¾åºç»æã
$$
L_{fm}=||E_{(x^l,y^l)~D^l}[D_k(y^{l}{\oplus}x^{l})]-E_{x^{u}{~}D^u}[D_k(S(x^{u}){\oplus}x^{u})]||
$$
c. self-trainingæå¤±ç¨äºä¿æçæå¨åå¤å«å¨ä¹é´çå¨æå¹³è¡¡: ä¸ºæ æ æ³¨çå¾çéæ©çæå¨(åå²ç½ç»$S$)è¾åºçæä¼çåå²ç»æ(å¯ä»¥éªè¿å¤å«å¨çç»æ)ä½ä¸ºæ æ³¨æ¥è¿è¡çç£è®­ç»ï¼å©ç¨å¤å«å¨è¾åºçè¯ååéæ©(è®¾å®éå¼)
$$
L_{st}=
\begin{cases}
-\sum_{h,w,c}y^*logS(x^u), &amp;amp;\text{if }D(S(x^u)){\ge}\gamma\&lt;br&gt;
0,&amp;amp; \text{otherwise}
\end{cases}
$$
å¶ä¸­ï¼$y^*$æ¯ç±$S(x^u)$äº§ççpseudo pixel-wise labels&lt;/p&gt;
&lt;p&gt;æç»ï¼æå¤±å½æ°ä¸ºï¼
$$
L_S=L_{ce}+{\lambda}_{fm}L_{fm}+{\lambda}_{st}L_{st}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;è®­ç»å¤å«ç½ç»$D$:
$$
L_{D}=E_{(x^l,y^l)~D^l}[\text{log}D(y^{l}{\oplus}x^{l})]+E_{x^{u}{~}D^u}[\text{log}(1-D(S(x^{u}){\oplus}x^{u}))]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multi-label Semi-supervised Classification&lt;/p&gt;
&lt;p&gt;ä½¿ç¨ Mean Teacher framework æ¥è¿è¡semi-supervised multilabel image classification.&lt;/p&gt;
&lt;p&gt;Student network $G_{\theta}$ åTeacher network $H_{{\theta}^{&#39;}}$çè¾å¥åå«ä¸ºåä¸å¼ å¾åçä¸åæ°å¨åçå¾åï¼Teacher networkçåæ°æéæ¯Student networkåæ°çexponential moving average. Student networkçæå¤±å½æ°ä¸ºï¼
$$
L_{MT}=-\sum_{c}z^{l}(c)\text{log}(G_{\theta}(x^l)(c))+\lambda_{cons}||G_{\theta}(x^{&#39;(u,l)}-H_{{\theta}^{&#39;}}(x^{(u,l)})||^2
$$
å¶ä¸­ï¼$x$å$x^{&#39;}$æ¯åä¸å¼ å¾ççä¸åæ°å¨ï¼$z^{l}$æ¯çå®æ ç­¾çmulti-hot vector.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Network Fusion&lt;/p&gt;
&lt;p&gt;ä¸¤ä¸ªbranchæ¯åå¼è®­ç»çï¼æç»éç¨Network Fusionçæ¹å¼è¿è¡èåä»¥å¾å°æåçç»ä¸ç»æ,ææ³ä¸ºæ ¹æ®é¢æµæå¾å¾çä¸åºç°çç±»å«æ¦çç¼©æ¾å¶å¯¹åºçåå²å¾ä¸çæ¦çï¼&lt;/p&gt;
&lt;p&gt;$$
S(x)_{c}=
\begin{cases}
0, &amp;amp;\text{if }G(x)_c{\le}\tau\&lt;br&gt;
S(x)_c,&amp;amp; \text{otherwise}
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;$S(x)_{c}$æ¯ç¬¬$c$ç±»çåå²å¾,$G(x_c)$æ¯MLMT-branchçsoft output, $\tau=0.2$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;supplement-mean-teacher-framework&#34;&gt;Supplement. Mean Teacher Framework&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Tarvainen A , Valpola H . Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. NIPS 2017.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Semi-supervisedéå¸¸å¯å°æ°æ®åä¸ºå¸¦æ ç­¾ãä¸å¸¦æ ç­¾ä¸¤é¨åãMean Teacher Frameworkéå¸¸æ¯æå¯¹æ æ ç­¾æ°æ®è¿è¡ä¸åå½¢å¼çæ°æ®å¢å¹¿ï¼åå«éå¥teacher modelåstudent model (teacher modelåstudent modeléç¨ç¸åçæ¨¡åæ¶æ)ï¼çº¦æteacher modelåstudent modelå¯¹åä¸æ ·æ¬ä¸åå¢å¹¿å½¢å¼å·æä¸è´æ§è¾åºã&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;meanTeach.png&#34; alt=&#34;image-20201029205629557&#34;&gt;&lt;/p&gt;
&lt;p&gt;å¨è®­ç»è¿ç¨ä¸­ï¼é¦åå©ç¨æ æ³¨æ°æ®å¯¹student modelè¿è¡çç£å­¦ä¹ ï¼éåå©ç¨æ æ æ³¨æ°æ®æ¥ä¼åteacher modelï¼å·ä½æµç¨å¦ä¸ï¼&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;å©ç¨studentæ¨¡åå¯¹å¾åè¿è¡é¢æµå¾å°$y_i$,ç¶åå©ç¨teacher modelé¢æµå¾å°$y_{i}^{â}$ï¼&lt;/li&gt;
&lt;li&gt;çº¦æé¢æµç»æ$y_i$å$y_i^{&#39;}$çä¸è´æ§($mseLoss$æè$KLLoss$)ï¼å¹¶ç»åææ æ³¨çå¾åè®¡ç®çç£æå¤±($crossEntropyLoss$)ï¼å°ä¸¤é¨åçæå¤±å½æ°è¿è¡æ¢¯åº¦åä¼ ç¨äºæ´æ°student modelçåæ°&lt;/li&gt;
&lt;li&gt;teacher modelæ¨¡åçåæ°éè¿è®¡ç®ä¸studentæ¨¡ååæ°çæ»å¨å¹³åå¾å°ã&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\theta_{t}^{&#39;} = {\alpha}{\theta}_{t-1}^{&#39;}+(1-{\alpha}){\theta}_t
$$&lt;/p&gt;
&lt;p&gt;â		å¶ä¸­${\theta}_t$ä¸ºstudent modelçåæ°ï¼${\theta}_t^{&#39;}$ä¸ºteacher modelçåæ°ãæåteacher modelå°ä½ä¸ºæç»çé¢æµæ¨¡åã&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Constrain Latent Space for Schizophrenia Classification via Dual Space Mapping Net</title>
      <link>/publication/miccai2020_wyshi/</link>
      <pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/publication/miccai2020_wyshi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt; | 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>/project/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>/project/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
